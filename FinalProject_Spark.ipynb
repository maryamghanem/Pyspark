{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/03 13:47:58 WARN Utils: Your hostname, DESKTOP-26AECPL resolves to a loopback address: 127.0.1.1; using 192.168.220.1 instead (on interface eth1)\n",
      "24/09/03 13:47:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/03 13:47:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.220.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Final_Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa7e8114110>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Final_Project\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/03 13:48:13 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+-------+\n",
      "| id|         City|State|Country|\n",
      "+---+-------------+-----+-------+\n",
      "|ABE|    Allentown|   PA|    USA|\n",
      "|ABI|      Abilene|   TX|    USA|\n",
      "|ABQ|  Albuquerque|   NM|    USA|\n",
      "|ABR|     Aberdeen|   SD|    USA|\n",
      "|ABY|       Albany|   GA|    USA|\n",
      "|ACK|    Nantucket|   MA|    USA|\n",
      "|ACT|         Waco|   TX|    USA|\n",
      "|ACV|       Eureka|   CA|    USA|\n",
      "|ACY|Atlantic City|   NJ|    USA|\n",
      "|ADQ|       Kodiak|   AK|    USA|\n",
      "|AEX|   Alexandria|   LA|    USA|\n",
      "|AGS|      Augusta|   GA|    USA|\n",
      "|AHN|       Athens|   GA|    USA|\n",
      "|AIA|     Alliance|   NE|    USA|\n",
      "|AKN|  King Salmon|   AK|    USA|\n",
      "|ALB|       Albany|   NY|    USA|\n",
      "|ALO|     Waterloo|   IA|    USA|\n",
      "|ALS|      Alamosa|   CO|    USA|\n",
      "|ALW|  Walla Walla|   WA|    USA|\n",
      "|AMA|     Amarillo|   TX|    USA|\n",
      "+---+-------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.parquet(\"/home/codebind/spark/Final Project_Spark/VertFinalExam\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+---+---+\n",
      "| tripid|delay|distance|src|dst|\n",
      "+-------+-----+--------+---+---+\n",
      "|1010630|  -10|     928|RSW|EWR|\n",
      "|1021029|   87|     974|RSW|ORD|\n",
      "|1021346|    0|     928|RSW|EWR|\n",
      "|1021044|   18|     928|RSW|EWR|\n",
      "|1021730|   29|     748|RSW|IAH|\n",
      "|1020535|  605|     974|RSW|ORD|\n",
      "|1021820|   71|     974|RSW|ORD|\n",
      "|1021743|    0|     928|RSW|EWR|\n",
      "|1022017|    0|     928|RSW|EWR|\n",
      "|1020600|   -2|     748|RSW|IAH|\n",
      "|1021214|   29|     891|RSW|CLE|\n",
      "|1020630|   -5|     928|RSW|EWR|\n",
      "|1031029|   13|     974|RSW|ORD|\n",
      "|1031346|  279|     928|RSW|EWR|\n",
      "|1031740|   29|     748|RSW|IAH|\n",
      "|1030535|    0|     974|RSW|ORD|\n",
      "|1031808|   -3|     974|RSW|ORD|\n",
      "|1031516|   -2|    1396|RSW|DEN|\n",
      "|1032017|   14|     928|RSW|EWR|\n",
      "|1031214|   17|     891|RSW|CLE|\n",
      "+-------+-----+--------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.parquet(\"/home/codebind/spark/Final Project_Spark/EdgesFinalExam\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "vertices_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "])\n",
    "\n",
    "edges_schema = StructType([\n",
    "    StructField(\"tripid\", IntegerType(), True),\n",
    "    StructField(\"delay\", IntegerType(), True),\n",
    "    StructField(\"distance\", IntegerType(), True),\n",
    "    StructField(\"src\", StringType(), True),\n",
    "    StructField(\"dst\", StringType(), True)\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vertices = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"header\", \"False\") \\\n",
    "    .schema(vertices_schema) \\\n",
    "    .load(\"/home/codebind/spark/Final Project_Spark/VertFinalExam\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"header\", \"False\") \\\n",
    "    .schema(edges_schema) \\\n",
    "    .load(\"/home/codebind/spark/Final Project_Spark/EdgesFinalExam\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_edges_processed = df_edges.withColumn(\n",
    "    \"delay_category\",\n",
    "    when(df_edges[\"delay\"] < 0, \"Early\")\n",
    "    .when(df_edges[\"delay\"] > 0, \"Late\")\n",
    "    .otherwise(\"OnTime\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vertices_processed = df_vertices.filter(df_vertices[\"state\"] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/03 13:48:16 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "vertices_query = df_vertices_processed.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/home/codebind/spark/Final Project_Spark/MyFirstFolder\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/codebind/spark/Final Project_Spark/MyFirstFolder/vertices_checkpoint\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/03 13:48:16 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "edges_query = df_edges_processed.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/home/codebind/spark/Final Project_Spark/MySecondFolder\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/codebind/spark/Final Project_Spark/MySecondFolder/edges_checkpoint\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+-------+\n",
      "| id|         City|State|Country|\n",
      "+---+-------------+-----+-------+\n",
      "|ABE|    Allentown|   PA|    USA|\n",
      "|ABI|      Abilene|   TX|    USA|\n",
      "|ABQ|  Albuquerque|   NM|    USA|\n",
      "|ABR|     Aberdeen|   SD|    USA|\n",
      "|ABY|       Albany|   GA|    USA|\n",
      "|ACK|    Nantucket|   MA|    USA|\n",
      "|ACT|         Waco|   TX|    USA|\n",
      "|ACV|       Eureka|   CA|    USA|\n",
      "|ACY|Atlantic City|   NJ|    USA|\n",
      "|ADQ|       Kodiak|   AK|    USA|\n",
      "|AEX|   Alexandria|   LA|    USA|\n",
      "|AGS|      Augusta|   GA|    USA|\n",
      "|AHN|       Athens|   GA|    USA|\n",
      "|AIA|     Alliance|   NE|    USA|\n",
      "|AKN|  King Salmon|   AK|    USA|\n",
      "|ALB|       Albany|   NY|    USA|\n",
      "|ALO|     Waterloo|   IA|    USA|\n",
      "|ALS|      Alamosa|   CO|    USA|\n",
      "|ALW|  Walla Walla|   WA|    USA|\n",
      "|AMA|     Amarillo|   TX|    USA|\n",
      "+---+-------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static_vertices_df = spark.read.format(\"parquet\") \\\n",
    "    .load(\"/home/codebind/spark/Final Project_Spark/MyFirstFolder\")\n",
    "\n",
    "static_vertices_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+---+---+--------------+\n",
      "| tripid|delay|distance|src|dst|delay_category|\n",
      "+-------+-----+--------+---+---+--------------+\n",
      "|1010630|  -10|     928|RSW|EWR|         Early|\n",
      "|1021029|   87|     974|RSW|ORD|          Late|\n",
      "|1021346|    0|     928|RSW|EWR|        OnTime|\n",
      "|1021044|   18|     928|RSW|EWR|          Late|\n",
      "|1021730|   29|     748|RSW|IAH|          Late|\n",
      "|1020535|  605|     974|RSW|ORD|          Late|\n",
      "|1021820|   71|     974|RSW|ORD|          Late|\n",
      "|1021743|    0|     928|RSW|EWR|        OnTime|\n",
      "|1022017|    0|     928|RSW|EWR|        OnTime|\n",
      "|1020600|   -2|     748|RSW|IAH|         Early|\n",
      "|1021214|   29|     891|RSW|CLE|          Late|\n",
      "|1020630|   -5|     928|RSW|EWR|         Early|\n",
      "|1031029|   13|     974|RSW|ORD|          Late|\n",
      "|1031346|  279|     928|RSW|EWR|          Late|\n",
      "|1031740|   29|     748|RSW|IAH|          Late|\n",
      "|1030535|    0|     974|RSW|ORD|        OnTime|\n",
      "|1031808|   -3|     974|RSW|ORD|         Early|\n",
      "|1031516|   -2|    1396|RSW|DEN|         Early|\n",
      "|1032017|   14|     928|RSW|EWR|          Late|\n",
      "|1031214|   17|     891|RSW|CLE|          Late|\n",
      "+-------+-----+--------+---+---+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static_edges_df = spark.read.format(\"parquet\") \\\n",
    "    .load(\"/home/codebind/spark/Final Project_Spark/MySecondFolder\")\n",
    "\n",
    "static_edges_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
